{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from Tars.distributions import Normal, Bernoulli\n",
    "from Tars.losses.divergences import KullbackLeibler\n",
    "from Tars.models import VAE\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shepardmetzler import ShepardMetzler, Scene, transform_viewpoint\n",
    "from conv_lstm import Conv2dLSTMCell\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TowerRepresentation\n",
    "class Representation(nn.Module):\n",
    "    def __init__(self, n_channels, v_dim, r_dim=256, pool=True):\n",
    "        \"\"\"\n",
    "        Network that generates a condensed representation\n",
    "        vector from a joint input of image and viewpoint.\n",
    "\n",
    "        Employs the tower/pool architecture described in the paper.\n",
    "\n",
    "        :param n_channels: number of color channels in input image\n",
    "        :param v_dim: dimensions of the viewpoint vector\n",
    "        :param r_dim: dimensions of representation\n",
    "        :param pool: whether to pool representation\n",
    "        \"\"\"\n",
    "        super(Representation, self).__init__()\n",
    "        # Final representation size\n",
    "        self.r_dim = k = r_dim\n",
    "        self.pool = pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, k, kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(k, k, kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(k, k//2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(k//2, k, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(k + v_dim, k, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(k + v_dim, k//2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(k//2, k, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(k, k, kernel_size=1, stride=1)\n",
    "\n",
    "        self.avgpool  = nn.AvgPool2d(k//16)\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        \"\"\"\n",
    "        Send an (image, viewpoint) pair into the\n",
    "        network to generate a representation\n",
    "        :param x: image\n",
    "        :param v: viewpoint (x, y, z, cos(yaw), sin(yaw), cos(pitch), sin(pitch))\n",
    "        :return: representation\n",
    "        \"\"\"\n",
    "        # Increase dimensions\n",
    "        v = v.view(v.size(0), -1, 1, 1)\n",
    "        v = v.repeat(1, 1, self.r_dim // 16, self.r_dim // 16)\n",
    "\n",
    "        # First skip-connected conv block\n",
    "        skip_in  = F.relu(self.conv1(x))\n",
    "        skip_out = F.relu(self.conv2(skip_in))\n",
    "\n",
    "        x = F.relu(self.conv3(skip_in))\n",
    "        x = F.relu(self.conv4(x)) + skip_out\n",
    "\n",
    "        # Second skip-connected conv block (merged)\n",
    "        skip_in = torch.cat([x, v], dim=1)\n",
    "        skip_out  = F.relu(self.conv5(skip_in))\n",
    "\n",
    "        x = F.relu(self.conv6(skip_in))\n",
    "        x = F.relu(self.conv7(x)) + skip_out\n",
    "\n",
    "        r = F.relu(self.conv8(x))\n",
    "\n",
    "        if self.pool:\n",
    "            r = self.avgpool(r)\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorCore(nn.Module):\n",
    "    def __init__(self, v_dim, r_dim, z_dim, h_dim, SCALE):\n",
    "        super(GeneratorCore, self).__init__()\n",
    "        self.core = Conv2dLSTMCell(v_dim + r_dim + z_dim, h_dim, kernel_size=5, stride=1, padding=2)\n",
    "        self.upsample = nn.ConvTranspose2d(h_dim, h_dim, kernel_size=SCALE, stride=SCALE, padding=0)\n",
    "        \n",
    "    def forward(self, z, v, r, h_g, c_g, u):\n",
    "        h_g, c_g =  self.core(torch.cat([z, v, r], dim=1), [h_g, c_g])\n",
    "        u = self.upsample(h_g) + u\n",
    "        return h_g, c_g, u\n",
    "\n",
    "\n",
    "class InferenceCore(nn.Module):\n",
    "    def __init__(self, x_dim, v_dim, r_dim, h_dim):\n",
    "        super(InferenceCore, self).__init__()\n",
    "        self.core = Conv2dLSTMCell(h_dim + x_dim + v_dim + r_dim, h_dim, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, x, v, r, h_g, h_e, c_e):\n",
    "        h_e, c_e = self.core(torch.cat([h_g, x, v, r], dim=1), [h_e, c_e])\n",
    "        return h_e, c_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Normal):\n",
    "    def __init__(self, x_dim, h_dim):\n",
    "        super(Generator, self).__init__(cond_var=[\"u\", \"sigma\"],var=[\"x_q\"])\n",
    "        self.eta_g = nn.Conv2d(h_dim, x_dim, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    # TODO; enable sigma annealing\n",
    "    def forward(self, u, sigma):\n",
    "        mu = torch.sigmoid(self.eta_g(u))\n",
    "        return {\"loc\":mu, \"scale\":sigma}\n",
    "\n",
    "class Prior(Normal):\n",
    "    def __init__(self, z_dim, h_dim):\n",
    "        super(Prior, self).__init__(cond_var=[\"h_g\"],var=[\"z\"])\n",
    "        self.z_dim = z_dim\n",
    "        self.eta_pi = nn.Conv2d(h_dim, 2*z_dim, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, h_g):\n",
    "        mu, std = torch.split(self.eta_pi(h_g), self.z_dim, dim=1)\n",
    "        return {\"loc\":mu ,\"scale\":F.softplus(std)}\n",
    "    \n",
    "class Inference(Normal):\n",
    "    def __init__(self, z_dim, h_dim):\n",
    "        super(Inference, self).__init__(cond_var=[\"h_i\"],var=[\"z\"])\n",
    "        self.z_dim = z_dim\n",
    "        self.eta_e = nn.Conv2d(h_dim, 2*z_dim, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, h_i):\n",
    "        mu, std = torch.split(self.eta_e(h_i), self.z_dim, dim=1)\n",
    "        return {\"loc\":mu, \"scale\":F.softplus(std)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQN(nn.Module):\n",
    "    def __init__(self, x_dim, v_dim, r_dim, h_dim, z_dim, L, SCALE):\n",
    "        super(GQN, self).__init__()\n",
    "        self.L = L\n",
    "        self.h_dim = h_dim\n",
    "        self.SCALE = SCALE\n",
    "        \n",
    "        self.representation = Representation(x_dim, v_dim, r_dim)\n",
    "        self.generator_core = GeneratorCore(v_dim, r_dim, z_dim, h_dim, self.SCALE)\n",
    "        self.inference_core = InferenceCore(x_dim, v_dim, r_dim, h_dim)\n",
    "        \n",
    "        self.upsample   = nn.ConvTranspose2d(h_dim, h_dim, kernel_size=SCALE, stride=SCALE, padding=0)\n",
    "        self.downsample = nn.Conv2d(x_dim, x_dim, kernel_size=SCALE, stride=SCALE, padding=0)\n",
    "        \n",
    "        self.pi = Prior(z_dim, h_dim).to(device)\n",
    "        self.q = Inference(z_dim, h_dim).to(device)\n",
    "        self.g = Generator(x_dim, h_dim).to(device)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, images, viewpoints, sigma):\n",
    "        # Number of context datapoints to use for representation\n",
    "        batch_size, m, *_ = viewpoints.size()\n",
    "\n",
    "        # Sample random number of views and generate representation\n",
    "        n_views = random.randint(2, m-1)\n",
    "\n",
    "        indices = torch.randperm(m)\n",
    "        representation_idx, query_idx = indices[:n_views], indices[n_views]\n",
    "\n",
    "        x, v = images[:, representation_idx], viewpoints[:, representation_idx]\n",
    "\n",
    "        # Merge batch and view dimensions.\n",
    "        _, _, *x_dims = x.size()\n",
    "        _, _, *v_dims = v.size()\n",
    "\n",
    "        x = x.view((-1, *x_dims))\n",
    "        v = v.view((-1, *v_dims))\n",
    "\n",
    "        # representation generated from input images\n",
    "        # and corresponding viewpoints\n",
    "        phi = self.representation(x, v)\n",
    "\n",
    "        # Seperate batch and view dimensions\n",
    "        _, *phi_dims = phi.size()\n",
    "        phi = phi.view((batch_size, n_views, *phi_dims))\n",
    "\n",
    "        # sum over view representations\n",
    "        r = torch.sum(phi, dim=1)\n",
    "\n",
    "        # Use random (image, viewpoint) pair in batch as query\n",
    "        x_q, v_q = images[:, query_idx], viewpoints[:, query_idx]\n",
    "        \n",
    "        \n",
    "        batch_size, _, h, w = x_q.size()\n",
    "        kl = 0\n",
    "\n",
    "        # Increase dimensions\n",
    "        v_q = v_q.view(batch_size, -1, 1, 1).repeat(1, 1, h//self.SCALE, w//self.SCALE)\n",
    "        if r.size(2) != h//self.SCALE:\n",
    "            r = r.repeat(1, 1, h//self.SCALE, w//self.SCALE)\n",
    "        \n",
    "        # Reset hidden state\n",
    "        hidden_g = x_q.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "        hidden_i = x_q.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "\n",
    "        # Reset cell state\n",
    "        cell_g = x_q.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "        cell_i = x_q.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "\n",
    "        u = x_q.new_zeros((batch_size, self.h_dim, h, w))\n",
    "        \n",
    "        x_q_downsampled = self.downsample(x_q)\n",
    "        \n",
    "        kls = 0\n",
    "        for _ in range(self.L):    \n",
    "            # kl\n",
    "            z = self.q.sample({\"h_i\": hidden_i})[\"z\"]\n",
    "            kl = KullbackLeibler(self.q, self.pi)\n",
    "            kl_estimated = kl.estimate({\"h_i\":hidden_i, \"h_g\":hidden_g})\n",
    "            kls += kl_estimated\n",
    "            # update state\n",
    "            hidden_i, cell_i = self.inference_core(x_q_downsampled, v_q, r, hidden_g, hidden_i, cell_i)\n",
    "            hidden_g, cell_g, u = self.generator_core(z, v_q, r, hidden_g, cell_g, u)\n",
    "        \n",
    "        x_sample = self.g.sample({\"u\": u, \"sigma\":sigma})\n",
    "        x_nll = -self.g.log_likelihood(x_sample)\n",
    "        kls = torch.sum(kls.view(batch_size, -1),dim=1)\n",
    "        loss = x_nll + kls\n",
    "        return loss, x_q, x_sample['x_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xDim=3\n",
    "vDim=7\n",
    "rDim=256\n",
    "hDim=128\n",
    "zDim=64\n",
    "L=12\n",
    "SCALE = 4 # Scale of image generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gqn=GQN(xDim,vDim,rDim,hDim,zDim, L, SCALE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 351/50569 [05:42<13:35:32,  1.03it/s]"
     ]
    }
   ],
   "source": [
    "# args\n",
    "train_data_dir = '/root/dataset/shepard_metzler_7_parts-torch/train'\n",
    "\n",
    "# for logging\n",
    "log_interval_num = 1000\n",
    "dir_name = str(datetime.datetime.now())\n",
    "log_dir = '/root/logs/'+ dir_name\n",
    "os.mkdir(log_dir)\n",
    "os.mkdir(log_dir+'/models')\n",
    "os.mkdir(log_dir+'/runs')\n",
    "\n",
    "# tensorboardX\n",
    "writer = SummaryWriter(log_dir=log_dir+'/runs')\n",
    "\n",
    "batch_size = 16\n",
    "gradient_steps = 2*(10**6)\n",
    "\n",
    "train_dataset = ShepardMetzler(root_dir=train_data_dir, target_transform=transform_viewpoint)\n",
    "\n",
    "# Pixel variance\n",
    "sigma_f, sigma_i = 0.7, 2.0\n",
    "\n",
    "# Learning rate\n",
    "mu_f, mu_i = 5*10**(-5), 5*10**(-4)\n",
    "mu, sigma = mu_f, sigma_f\n",
    "\n",
    "optimizer = torch.optim.Adam(gqn.parameters(), lr=mu)\n",
    "kwargs = {'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# Number of gradient steps\n",
    "s = 0\n",
    "while True:\n",
    "    for x, v in tqdm(loader):\n",
    "        x = x.to(device)\n",
    "        v = v.to(device)\n",
    "        loss, x_target, x_reconst = gqn(x, v, sigma)\n",
    "        loss = torch.mean(loss.view(batch_size, -1), dim=0)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        s += 1\n",
    "        # Anneal learning rate\n",
    "        mu = max(mu_f + (mu_i - mu_f)*(1 - s/(1.6 * 10**6)), mu_f)\n",
    "        optimizer.lr = mu * math.sqrt(1 - 0.999**s)/(1 - 0.9**s)\n",
    "        # Anneal pixel variance\n",
    "        sigma = max(sigma_f + (sigma_i - sigma_f)*(1 - s/(2 * 10**5)), sigma_f)\n",
    "        \n",
    "        # Keep a checkpoint every n steps\n",
    "        if s % log_interval_num == 0:\n",
    "            torch.save(gqn, log_dir + \"/models/model-{}.pt\".format(s))\n",
    "            writer.add_scalar('train_loss', loss, s)  \n",
    "            writer.add_image('target_image', x_target[0], s)\n",
    "            writer.add_image('target_reconst', x_reconst[0], s)\n",
    "            \n",
    "        if s >= gradient_steps:\n",
    "            break\n",
    "    \n",
    "    if s >= gradient_steps:\n",
    "        torch.save(gqn, log_dir + \"/models/model-final.pt\")\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
